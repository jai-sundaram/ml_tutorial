{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jai-sundaram/ml_tutorial/blob/main/my_data_preprocessing_tools.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37puETfgRzzg"
      },
      "source": [
        "# Data Preprocessing Tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoRP98MpR-qj"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##numpy helps us work with arrays\n",
        "import numpy as np\n",
        "#matplotlib helps us plot numerous charts, particularly pylpot which allows us to plot nice charts\n",
        "import matplotlib.pyplot as plt\n",
        "#pandas helps import the dataset, create the matrix, and independent variable vector\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "#to add the csv to the google colab notebook, click on the Files folder, click on the Upload button, and then upload the file"
      ],
      "metadata": {
        "id": "h0zV694EObXk"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RopL7tUZSQkT"
      },
      "source": [
        "## Importing the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Owtlo2ME2kNc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#reading the values of the data set and creating a data frame\n",
        "#make sure the path for the csv is correct\n",
        "#find the csv in the folder structure, and click on copy path\n",
        "dataset = pd.read_csv('/content/Data.csv')\n",
        "#creating two additional entities\n",
        "#creating a matrix of features and a dependent variable vector\n",
        "\n",
        "#the features are the columns you are using to predict the dependent variable\n",
        "#the dependent variable is the last column, the thing you are predicting\n",
        "#in other terms, the features are the independent variables\n",
        "\n",
        "##in this case, the country, age, and salary are the features/dependent variabels\n",
        "##the fact if they purchased or not is the dependent variable\n",
        "\n",
        "#formatting wise, most datasets will have the features/dependent variables in the first few columns, and the dependent variable in the last column\n",
        "#to get the create the matrix, using the iloc method\n",
        "#using the indexes of the columns\n",
        "#getting all the rows, so just say \":\", which is everything, since there is no specified range\n",
        "#getting the first three columns, so basically getting all rows except the last one\n",
        "#the last one is -1, so just do :-1\n",
        "#including everything in the lower bound, excluding the upper bound which is -1, or the last value\n",
        "#adding .values, means that we are just simply taking the values\n",
        "x = dataset.iloc[:,:-1].values\n",
        "\n",
        "#doing the same for the dependent variable vector\n",
        "#only need to get one column, so not a range\n",
        "#just -1\n",
        "y= dataset.iloc[:, -1].values"
      ],
      "metadata": {
        "id": "eSCAIAmhRvsL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testing to make sure we correctly created the matrix and vector\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jo8Av7q1k1tA",
        "outputId": "52f13f1f-7c76-4fc8-89c5-63b7a59a9d48"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 nan]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' nan 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9efQ3wSQlUCR",
        "outputId": "c3af125e-0762-4d12-a489-4b11a880074d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Au8w70yAinTq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhfKXNxlSabC"
      },
      "source": [
        "## Taking care of missing data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#one way of dealing with missing data is to simply ignore it - this works if it is a large dataset and there are only a few things missing\n",
        "\n",
        "#however, if this is not a case, you can simply replace the missing entry with the average in that column\n",
        "\n",
        "# to do so, we will be using the scikit-learn library\n",
        "# in particular, we will be using the SimpleImputer class\n",
        "\n",
        "#first, we need to import it\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "#create an object of it\n",
        "#the first argument is the thing we are replacing, which are the empty/missing values\n",
        "#the second argument is the thing we are replacing it with, which in this case is the meann\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy = 'mean')\n",
        "\n",
        "#use the fit method to connect the imputer to the matrix of features\n",
        "#it expects all the columns of the matrixes with numerical values, ONLY the ones with numerical values no string values\n",
        "#for only specify the age and salary columns as those are the only numerical\n",
        "#remember the upper bound is excluded, so it should be 3\n",
        "imputer.fit(x[:, 1:3])\n",
        "#use the transform method which will apply the transformation, which in this case is the replacement of the missing values\n",
        "#for the arguments do the same thing, just input the columns where some entries need to be replaced, which is again 2 and 3\n",
        "#the transform method actually returns the updated matrix, so just directly save it into those two columns\n",
        "x[:, 1:3] = imputer.transform(x[:, 1:3])"
      ],
      "metadata": {
        "id": "J7wAoo6soA_8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testing\n",
        "print(x)"
      ],
      "metadata": {
        "id": "FEr2YM85tBCd",
        "outputId": "ef7c0b6a-e997-4112-9917-9789e42c1ab2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 63777.77777777778]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' 38.77777777777778 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CriG6VzVSjcK"
      },
      "source": [
        "## Encoding categorical data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#need to turn categorical variables (strings) into numbers\n",
        "#One Hot Encoding helps us do this\n",
        "#In this case, One Hot Encoding will help us turn the country column into three different columns\n",
        "#there are three different countries/categories\n",
        "#One Hot Encoding creates a binary vector for each country\n",
        "#France would have the vector 100\n",
        "#Spain would have the vector 010\n",
        "#Germany would have the vector 001\n",
        "#no numerical order between the three countries\n",
        "\n",
        "#eventually we will have to replace the Purchased? column as well because those are also categorical values\n",
        "#before that tho, lets encode the Country column which is the independent variable that is categorical"
      ],
      "metadata": {
        "id": "2649IHyFBuI3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhSpdQWeSsFh"
      },
      "source": [
        "### Encoding the Independent Variable"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#we need to use two classes\n",
        "#The ColumnTransform class and the OneHotEncoder class\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "#first creating an object of the ColumnTransformer class\n",
        "#the first argument is what type of transformation we want to do and which indexes of the columns we want to transform\n",
        "#the second argument is the remainder, basically if we want to keep the columns that are not being encoded or not\n",
        "# need to mention what we are doing (encoding), the type of encoding which in this case is One Hot Encoding, and to which columns\n",
        "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder = \"passthrough\")\n",
        "#need to connect it to the matrix\n",
        "#can just use one method to both fit and transform\n",
        "#for training, this matrix of features is expected to be in the form of a numpy array\n",
        "#need to force it to be in the form of a numpy array, so use numpy's array method\n",
        "#save the resulting matrix to the current one, basically updating it\n",
        "x = np.array(ct.fit_transform(x))\n"
      ],
      "metadata": {
        "id": "_l1pV8yaFas4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXh8oVSITIc6"
      },
      "source": [
        "### Encoding the Dependent Variable"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#need to encode the dependent variable\n",
        "#will be using the LabelEncoder class\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "#no arguments\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "#converts the Yes/Nos to numerical values\n",
        "#dont need a numpy array, becuse this is a dependent variable vector which is expected by the future machine learning models"
      ],
      "metadata": {
        "id": "NlWmgtSE1o8h"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testing\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37-2rTtE2a0m",
        "outputId": "a625a6fd-6062-4d74-c5f1-b9c4879ac00d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 0 0 1 1 0 1 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qb_vcgm3qZKW"
      },
      "source": [
        "## Splitting the dataset into the Training set and Test set"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#you first split the dataset into the training set and the test set\n",
        "#the training set is used to train the machine learning model on existing observations\n",
        "#the test set is used to evaluate the performance of the model on new observations\n",
        "#feature scaling consists of scaling your features to make sure they all take values in the same scale\n",
        "#dont want one feature to dominate another\n",
        "#we want to do feature scaling after splitting the data so that we dont accidentally touch the test set's data\n",
        "\n",
        "\n",
        "#Splitting the dataset\n",
        "#using train-test split function from the model selectionmodule in scikitlearn\n",
        "#this will create 4 seperate sets\n",
        "#there will be two pairs of matrix of features and the dependnet variable vector for the both the training set and test set\n",
        "#will get:\n",
        "#xtrain - matrix of features for the training set\n",
        "#xtest - matrix of features for the test set\n",
        "#ytrain - dependent variable vector for training set\n",
        "#ytest - dependent variable vector for test set\n",
        "\n",
        "#future ml models will expect this format as input\n",
        "#expects xtrain and ytrain for training\n",
        "#expects xtest and ytest for testing\n",
        "from sklearn.model_selection import train_test_split\n",
        "#first parameter is the matrix of features\n",
        "#second parameter is the dependent variable vector\n",
        "#third parameter is the split size of the test set\n",
        "#need more in the training set and less in the test set, so the model has more chance to understand the correlation\n",
        "#recommend split: 80% training, 20% testing split\n",
        "#fourth parameter is the random_set\n",
        "#for teaching purposes, we want to make sure the random factors are the same, so just set it equal to 1\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 1)"
      ],
      "metadata": {
        "id": "9oeHnOJa3ll7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testing\n",
        "print(x_train)"
      ],
      "metadata": {
        "id": "W08Qd_d5-OhP",
        "outputId": "a83c5e94-1150-4dc6-b721-8fef2403aa38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0 0.0 1.0 38.77777777777778 52000.0]\n",
            " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
            " [1.0 0.0 0.0 44.0 72000.0]\n",
            " [0.0 0.0 1.0 38.0 61000.0]\n",
            " [0.0 0.0 1.0 27.0 48000.0]\n",
            " [1.0 0.0 0.0 48.0 79000.0]\n",
            " [0.0 1.0 0.0 50.0 83000.0]\n",
            " [1.0 0.0 0.0 35.0 58000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#testing\n",
        "print(x_test)"
      ],
      "metadata": {
        "id": "KlJfrpOt-Oq5",
        "outputId": "4389272f-1291-4aa5-9573-f914070a6df8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0 1.0 0.0 30.0 54000.0]\n",
            " [1.0 0.0 0.0 37.0 67000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#testing\n",
        "print(y_train)"
      ],
      "metadata": {
        "id": "QpTZNzdJ-PCA",
        "outputId": "cbba4112-7c43-488d-b436-d6ab1c8723e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 0 0 1 1 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#testing\n",
        "print(y_test)"
      ],
      "metadata": {
        "id": "1rQmi_8R-PMv",
        "outputId": "fb3df751-e5a3-4e43-cdae-ed754886ccb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpGqbS4TqkIR"
      },
      "source": [
        "## Feature Scaling"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#feature scaling is done so that some features are not dominated by the other features\n",
        "#do not have to apply feature scaling for all the models, just for\n",
        "#two types of feature scaling\n",
        "#standardization and normalizationn\n",
        "#standardization - subtracting each value of the feature by the mean of the all the values in the feature and then dividing by the standard deviation of the feature\n",
        "#all values will be around -3 and +3\n",
        "#normalization - subtracting each value by the min value of the feature dividied by (max-min)\n",
        "#all values will be between 0 and 1\n",
        "#standardization is recommended overall\n",
        "#normalization is usually only recommended for normal distribution\n",
        "#standardization works well all the time, since it works all the time, it will be more recommended\n",
        "#we have to apply it on two matrixes of features, xtrain and xtest seperately\n",
        "#scaler will be fixed to xtrain then we will transform xtrain, then we will apply feature scaling to xtest\n",
        "#not allowed to fit the feature scaling tool on xtest\n",
        "#first apply the normal standardization formula on xtrain\n",
        "#to apply feature scaling on xtest, get the standard deviation and mean of xtrain\n",
        "#then apply the formula to xtest using these mean and standardization values\n",
        "\n",
        "#using the StandardScaler class from the scikit-learn library\n",
        "#will perform standardization on the matrix of features in the training set, and matrix of features of the test set\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "#no parameters needed\n",
        "sc = StandardScaler()\n",
        "#we do not need to apply standardixation on the dummy variables, which are the variables created from encoding the categorical variables\n",
        "#since they already take values between -3 and 3, as they are either 1 or 0, there is nothing extra to be done with standaridzation\n",
        "#standardization will make it worse because you will lose the interpration of the variables\n",
        "#first we will fit the scaler on the training set\n",
        "#remember we will not be standardizing the dummy variables\n",
        "#so only the age and salary columns\n",
        "#taking all columns 3 and up\n",
        "#so basically only the age and salary\n",
        "#(remember this is the dependent variable vector)\n",
        "#we're going to compute the mean and the standard deviation of the training set first using the fit method\n",
        "#then use the transform method to actually apply the standardization formula\n",
        "#however we can use the fit_transform method to do both\n",
        "x_train[:, 3:] = sc.fit_transform(x_train[:, 3:])\n",
        "\n",
        "#features of the test set will be standardized using the same scalar used in the training set\n",
        "#we will only be using the transform method\n",
        "#if we applied fit_transform we would get a new scalar\n",
        "#keep the scalar from the training set, just apply the formula\n",
        "x_test[:, 3:] = sc.transform(x_test[:, 3:])\n"
      ],
      "metadata": {
        "id": "fcBSBjypnw-4"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testing\n",
        "print(x_train)"
      ],
      "metadata": {
        "id": "TRabqZvVw40d",
        "outputId": "5ac1f9b4-1c9d-49d8-e82b-332010ea2b0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0 0.0 1.0 -0.1915918438457856 -1.0781259408412427]\n",
            " [0.0 1.0 0.0 -0.014117293757057902 -0.07013167641635401]\n",
            " [1.0 0.0 0.0 0.5667085065333239 0.6335624327104546]\n",
            " [0.0 0.0 1.0 -0.3045301939022488 -0.30786617274297895]\n",
            " [0.0 0.0 1.0 -1.901801144700799 -1.4204636155515822]\n",
            " [1.0 0.0 0.0 1.1475343068237056 1.2326533634535488]\n",
            " [0.0 1.0 0.0 1.4379472069688966 1.5749910381638883]\n",
            " [1.0 0.0 0.0 -0.7401495441200352 -0.5646194287757336]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#testing\n",
        "print(x_test)"
      ],
      "metadata": {
        "id": "EdcFemhPw3D6",
        "outputId": "8a9565bc-c704-4010-af1b-2fd02eb64c9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0 1.0 0.0 30.0 54000.0]\n",
            " [1.0 0.0 0.0 37.0 67000.0]]\n"
          ]
        }
      ]
    }
  ]
}