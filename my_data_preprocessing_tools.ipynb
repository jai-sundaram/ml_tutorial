{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jai-sundaram/ml_tutorial/blob/main/my_data_preprocessing_tools.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37puETfgRzzg"
      },
      "source": [
        "# Data Preprocessing Tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoRP98MpR-qj"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##numpy helps us work with arrays\n",
        "import numpy as np\n",
        "#matplotlib helps us plot numerous charts, particularly pylpot which allows us to plot nice charts\n",
        "import matplotlib.pyplot as plt\n",
        "#pandas helps import the dataset, create the matrix, and independent variable vector\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "#to add the csv to the google colab notebook, click on the Files folder, click on the Upload button, and then upload the file"
      ],
      "metadata": {
        "id": "h0zV694EObXk"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RopL7tUZSQkT"
      },
      "source": [
        "## Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#reading the values of the data set and creating a data frame\n",
        "#make sure the path for the csv is correct\n",
        "#find the csv in the folder structure, and click on copy path\n",
        "dataset = pd.read_csv('/Data.csv')\n",
        "#creating two additional entities\n",
        "#creating a matrix of features and a dependent variable vector\n",
        "\n",
        "#the features are the columns you are using to predict the dependent variable\n",
        "#the dependent variable is the last column, the thing you are predicting\n",
        "#in other terms, the features are the independent variables\n",
        "\n",
        "##in this case, the country, age, and salary are the features/dependent variabels\n",
        "##the fact if they purchased or not is the dependent variable\n",
        "\n",
        "#formatting wise, most datasets will have the features/dependent variables in the first few columns, and the dependent variable in the last column\n",
        "#to get the create the matrix, using the iloc method\n",
        "#using the indexes of the columns\n",
        "#getting all the rows, so just say \":\", which is everything, since there is no specified range\n",
        "#getting the first three columns, so basically getting all rows except the last one\n",
        "#the last one is -1, so just do :-1\n",
        "#including everything in the lower bound, excluding the upper bound which is -1, or the last value\n",
        "#adding .values, means that we are just simply taking the values\n",
        "x = dataset.iloc[:,:-1].values\n",
        "\n",
        "#doing the same for the dependent variable vector\n",
        "#only need to get one column, so not a range\n",
        "#just -1\n",
        "y= dataset.iloc[:, -1].values"
      ],
      "metadata": {
        "id": "eSCAIAmhRvsL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testing to make sure we correctly created the matrix and vector\n",
        "print(x)"
      ],
      "metadata": {
        "id": "Jo8Av7q1k1tA",
        "outputId": "9d88dafe-321c-4893-cb2e-25a4ed645ab1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 nan]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' nan 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y)"
      ],
      "metadata": {
        "id": "9efQ3wSQlUCR",
        "outputId": "93bb5bb7-dddf-4a46-85de-32a67b2f8bd6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Au8w70yAinTq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhfKXNxlSabC"
      },
      "source": [
        "## Taking care of missing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CriG6VzVSjcK"
      },
      "source": [
        "## Encoding categorical data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhSpdQWeSsFh"
      },
      "source": [
        "### Encoding the Independent Variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXh8oVSITIc6"
      },
      "source": [
        "### Encoding the Dependent Variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qb_vcgm3qZKW"
      },
      "source": [
        "## Splitting the dataset into the Training set and Test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpGqbS4TqkIR"
      },
      "source": [
        "## Feature Scaling"
      ]
    }
  ]
}