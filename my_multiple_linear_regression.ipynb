{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jai-sundaram/ml_tutorial/blob/main/my_multiple_linear_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CazISR8X_HUG"
      },
      "source": [
        "# Multiple Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#equation for multiple linear regression\n",
        "#y = b0 + b1x1 + b2x2 +..... bnxn\n",
        "#y = dependent variable\n",
        "#b0 is the y-intercept\n",
        "#b1 is the first slope coefficient\n",
        "#x1 is the first independent variable\n",
        "#b2 is the second slope coefficient\n",
        "#x2 is the second indpendent variable\n",
        "#bn is the nth slope coefficient\n",
        "#xn is the nth independent variable\n",
        "\n",
        "#in multiple linear regression, there are multiple features/independent variable\n",
        "\n",
        "#assumptions of linear regression:\n",
        "#anscombe's quartet - have similair statistics, but look very different when graphed\n",
        "#highlight that you can't blindly apply linear regressioon\n",
        "#make sure that your dataset is fit for linear regression\n",
        "#the assumptions:\n",
        "#linearity - linear relationship between dependent and independent variable\n",
        "#homoscedasticity - equal variance, there should not be a cone-type shape on your shart (this means that variane is dependent on the independent variable)\n",
        "#multivariate normality - if you look along the lines, you want to see a normal distributuion of the data points\n",
        "#independence of observations (includes 'no autocorrelation') - there should be no pattern in our data (patterns indicate that rows are not indpendent, some rows are affecting other rows )#\n",
        "#lack of multicollinearity = predictors should not be correlated\n",
        "#outlier check (not an assumption just an extra check) - should we remove outliers before building the linear regression\n",
        "#in this course, we will these assumptions will all be correct for this section  '\n",
        "\n",
        "#encoding categorical variabes\n",
        "#to add a categorical independent variable/feature to regression model, we need to use dummy variables\n",
        "#in this case, lets say the categorical variable is that state - only two options are New York and California\n",
        "#need to create two new columns, a New York column and California column\n",
        "#when ever the state for the row is NY, the NY column would be 1 and the CA column would be 0\n",
        "#when the state column is California, NY column would be 0 and CA column would be 1\n",
        "#to add it the equation, add another piece 'b4d1', where b4 is the coefficient and d1 is the value of the new york column\n",
        "#if b1 is 1, the state is new york, if b1 is 0, state is california\n",
        "#the coefficient for california is by default included in b0\n",
        "#when it is ny, and you add b4d1, the coefficient is the difference between ny and cali, so are altering the default state of the equation from ca to ny\n",
        "\n",
        "#dummy variable trap\n",
        "#if we included the second dummy variable what would happen\n",
        "#the model would not be able to distinguish the effects of d1 from the effects of d2 (dummy variable 2)\n",
        "#this is called the dummy variable trap\n",
        "#to sum it up, always omit one dummy varaiab\n",
        "#for example if u have nine dummy variables, only include 8\n",
        "#if u have two sets of dummy variables, use the same rules. it is seperate\n",
        "\n",
        "#p value and statistical significance in hypothesis testing\n",
        "#lets taking the situtation of a coin teset, there are two possible hypothesis\n",
        "#h0 - this is a fair coin\n",
        "#h1 - this is an unfair coin\n",
        "#h0 is the original assumption, the null hypothesis\n",
        "#h1 is the alternative hypothesis\n",
        "#based on the expirement, we might come to a contradiction, the null hypothesis was wrong\n",
        "#lets say that we toss a coin 6 times in a row, and all 6 times it is tails\n",
        "#the probabilit\n",
        "y of this happening is about 1%\n",
        "#everytime you toss a coin and get tails, the probability of getting tails again drops\n",
        "#basically the p-value dropping\n",
        "#the probabiltiy of this happening in a universe where the null hypothesis is true\n",
        "#if it was a universe where the alternative hypothesis was true, the p-value trend wouldn't be the same, it wouldnt be dropping 3\n",
        "#like lets say in this unvierse both sides of the coin were tails, the probaility of getting tails would make sense to stay at 100$\n",
        "#however, in hypothesis testing, we assume that the nulll hypothesis is correct\n",
        "#statistical significance (alpha) is at 5%\n",
        "#when something is less than 5%, it is extremely unlikely to see at random, so we can reject the null hypothesis with 95% confidence\n",
        "\n",
        "\n",
        "#building robust multiple linear regression models\n",
        "#now that we have numerous potential predictors, we need to decide which ones to keep and which ones to throw out\n",
        "#we cannot keep all the predictors, it will lead to a bad model because it wouldn't be very reliable. furthermore, we need to understand the workings of the model, so having a lot of variables make it confusing to understand m\n",
        "\n",
        "#there are 5 methods of building models:\n",
        "# All in\n",
        "# Backwards elimination\n",
        "# Forward selection\n",
        "# Bidirectional elimination\n",
        "# Score comparison\n",
        "#------\n",
        "#stepwise regression refers to backwards elimination, forward selection,a nd bidirectional elimination\n",
        "#sometimes people imply bidirectional elimination when they say stepwise regression\n",
        "\n",
        "#all in\n",
        "#throwing in all the variables\n",
        "#if u know that each exact variables are the true predictors\n",
        "# OR, you might use it to prepapre for backwards elimination\n",
        "\n",
        "#backwards-elimination\n",
        "# Step 1: Select a significance level to stay in the model (by default, SL = 0.05)\n",
        "# Step 2: Fit the model with all possible predictors (all in approch)\n",
        "# Step 3: Consider the predictor with the highest p-value. If its p-value > SL, go to step 4. Otherwise, the model is finished\n",
        "# Step 4: Remove said predictor\n",
        "# Step 5: Refit the model without this predictor. Go back up  to step 3. (Repeating)\n",
        "\n",
        "#once all variables' p-value is less than SL, model is done, as said above\n",
        "\n",
        "#forward-selection\n",
        "# Step 1: Select a significance level (default SL: 0.05)\n",
        "# Step 2: Fit all simple linear regression models (model with one independent variable). Select the one with the lowest p-value\n",
        "# Step 3: Add one extra predictor to the model\n",
        "# Step 4: Look for the predictor with the smallest p-value. If the p-value < SL, go back up to step 3. Otherwise the model is finished. Take the model that was exactly previous to the finished one, that is the final model.\n",
        "\n",
        "#bi-directional elimination\n",
        "# Step 1: Select a significance level to enter and significance level to stay (default: 0.05 for both)\n",
        "# Step 2: Perform step 2/3 of the forward selection (new variables must have p-value > slenter to enter)\n",
        "# Step 3: Perform all steps of backward elimination (old variables must have p <slstay to stay)\n",
        "# Step 4: No new variables can be added and no old variables can be removed. Model is finished\n",
        "\n",
        "#all possible models (score comparison)\n",
        "# Step 1: Select a criterion of goodness of fit (eg. Akaike criterion)\n",
        "# Step 2: Construct all possible regression models: (2^n) - 1 combinations\n",
        "# Step 3: Select the one with the best criterion\n",
        "\n",
        "\n",
        "#will be using backwards elimination because it is the fastest one"
      ],
      "metadata": {
        "id": "GP5dIbl9G_ji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOyqYHTk_Q57"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgC61-ah_WIz"
      },
      "source": [
        "## Importing the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VadrvE7s_lS9"
      },
      "source": [
        "## Encoding categorical data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WemVnqgeA70k"
      },
      "source": [
        "## Splitting the dataset into the Training set and Test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-McZVsQBINc"
      },
      "source": [
        "## Training the Multiple Linear Regression model on the Training set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNkXL1YQBiBT"
      },
      "source": [
        "## Predicting the Test set results"
      ]
    }
  ]
}